{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2653a14f-2ae5-4488-b4b8-935fa705c1f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0: Defining helper functions...\n",
      "\n",
      "Step 1: Loading and preparing the WikiArt dataset (all classes)...\n",
      "Prepared image_df with 200111 entries from all classes.\n",
      "\n",
      "Step 2: Defining image transformations...\n",
      "Image transformations defined.\n",
      "\n",
      "Step 3: Creating custom datasets and data loaders...\n",
      "\n",
      "Splitting dataset into training, validation, and test sets...\n",
      "Training set: 140077 images\n",
      "Validation set: 30017 images\n",
      "Test set: 30017 images\n",
      "DataLoaders created for training, validation, and test sets.\n",
      "\n",
      "Step 4: Setting up the device and loading the pre-trained model...\n",
      "Using device: cuda\n",
      "Pre-trained model loaded and modified for fine-tuning.\n",
      "\n",
      "Step 5: Fine-tuning the model...\n",
      "\n",
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   4%|‚ñç         | 171/4378 [04:45<1:57:13,  1.67s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_17845/600915981.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    276\u001b[0m     \u001b[0mrunning_corrects\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 278\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Training\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    279\u001b[0m         \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/FINAL/lib/python3.7/site-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1194\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1195\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1196\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1197\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/FINAL/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    433\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/FINAL/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1066\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1067\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1068\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1069\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1070\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/FINAL/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1022\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_thread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_alive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1024\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1025\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1026\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/FINAL/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    870\u001b[0m         \u001b[0;31m#   (bool: whether successfully get data, any: data if successful else None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 872\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    873\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    874\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/FINAL/lib/python3.7/queue.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    177\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mremaining\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_empty\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mremaining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m             \u001b[0mitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_full\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnotify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/FINAL/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    298\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 300\u001b[0;31m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    301\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from io import BytesIO\n",
    "from PIL import Image, ImageFilter\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')  # Use a non-interactive backend suitable for script running\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm  # For progress bars\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import models, transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import faiss\n",
    "\n",
    "# Additional imports for label encoding and model evaluation\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "\n",
    "# Step 0: Define Helper Functions\n",
    "print(\"Step 0: Defining helper functions...\")\n",
    "\n",
    "def show_results(query_image, results, query_index, transformed=False):\n",
    "    \"\"\"\n",
    "    Displays the query image alongside its top matching results and saves the figure.\n",
    "\n",
    "    Parameters:\n",
    "    - query_image (PIL.Image.Image): PIL image of the query.\n",
    "    - results (pd.DataFrame): DataFrame containing the top matching artworks.\n",
    "    - query_index (int): Index of the query image.\n",
    "    - transformed (bool): Indicates if the query image was transformed.\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    print(\"\\nDisplaying results visually...\")\n",
    "    if transformed:\n",
    "        # Visual indication that the query image was transformed\n",
    "        query_image_display = query_image.filter(ImageFilter.GaussianBlur(radius=2))\n",
    "        query_image_display = query_image_display.rotate(10)  # Example transformation\n",
    "    else:\n",
    "        query_image_display = query_image\n",
    "\n",
    "    num_results = min(len(results), 5)\n",
    "    plt.figure(figsize=(5 * (num_results + 1), 5))\n",
    "\n",
    "    # Display Query Image\n",
    "    plt.subplot(1, num_results + 1, 1)\n",
    "    plt.imshow(query_image_display)\n",
    "    title = 'Query Image'\n",
    "    if transformed:\n",
    "        title += ' (Transformed)'\n",
    "    plt.title(title)\n",
    "    plt.axis('off')\n",
    "\n",
    "    # Display Matching Images\n",
    "    for i in range(num_results):\n",
    "        img_path = results.iloc[i]['image_path']\n",
    "        try:\n",
    "            img = Image.open(img_path).convert('RGB')\n",
    "            plt.subplot(1, num_results + 1, i + 2)\n",
    "            plt.imshow(img)\n",
    "            plt.title(f\"Match {i+1}\")\n",
    "            plt.axis('off')\n",
    "            print(f\"Loaded Match {i+1}: {img_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to load image {img_path}: {e}\")\n",
    "\n",
    "    output_dir = 'results'\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    output_path = os.path.join(output_dir, f'query_results_{query_index}.png')\n",
    "    plt.savefig(output_path)\n",
    "    plt.close()\n",
    "    print(f\"Results displayed and saved to '{output_path}'.\")\n",
    "\n",
    "def get_image_features_batch(model, img_tensors):\n",
    "    \"\"\"\n",
    "    Extracts and normalizes features from a batch of image tensors.\n",
    "\n",
    "    Parameters:\n",
    "    - model (torch.nn.Module): The feature extraction model.\n",
    "    - img_tensors (torch.Tensor): Batch of image tensors.\n",
    "\n",
    "    Returns:\n",
    "    - np.ndarray: Normalized feature vectors.\n",
    "    \"\"\"\n",
    "    print(\"  Extracting features from the batch...\")\n",
    "    with torch.no_grad():\n",
    "        features = model(img_tensors)\n",
    "        features = features.view(features.size(0), -1)  # Flatten\n",
    "        features = features / features.norm(dim=1, keepdim=True)\n",
    "    print(\"  Features extracted and normalized.\")\n",
    "    return features.cpu().numpy().astype('float32')\n",
    "\n",
    "def search_artwork_batch(query_features, index, k=5):\n",
    "    \"\"\"\n",
    "    Searches for the top k similar artworks using FAISS in batch.\n",
    "\n",
    "    Parameters:\n",
    "    - query_features (np.ndarray): Batch of feature vectors.\n",
    "    - index (faiss.Index): FAISS index.\n",
    "    - k (int): Number of top matches to retrieve.\n",
    "\n",
    "    Returns:\n",
    "    - distances (np.ndarray): Similarity scores.\n",
    "    - indices (np.ndarray): Indices of the top matches.\n",
    "    \"\"\"\n",
    "    print(f\"  Searching for top {k} similar artworks in batch...\")\n",
    "    distances, indices = index.search(query_features, k)\n",
    "    print(\"  Batch search completed.\")\n",
    "    return distances, indices\n",
    "\n",
    "def get_artwork_info_batch(indices, image_df):\n",
    "    \"\"\"\n",
    "    Retrieves artwork information based on the indices for a batch.\n",
    "\n",
    "    Parameters:\n",
    "    - indices (np.ndarray): Indices of the top matches.\n",
    "    - image_df (pd.DataFrame): DataFrame containing image metadata.\n",
    "\n",
    "    Returns:\n",
    "    - List[pd.DataFrame]: List of DataFrames for each query's top matches.\n",
    "    \"\"\"\n",
    "    print(\"  Retrieving artwork information for the batch...\")\n",
    "    try:\n",
    "        results = [image_df.iloc[idx].reset_index(drop=True) for idx in indices]\n",
    "        print(\"  Artwork information retrieved for the batch.\")\n",
    "    except Exception as e:\n",
    "        print(f\"  Error retrieving artwork information for the batch: {e}\")\n",
    "        results = [pd.DataFrame() for _ in range(indices.shape[0])]  # Return empty DataFrames on error\n",
    "    return results\n",
    "\n",
    "# Step 1: Load and Prepare the WikiArt Dataset (All Classes)\n",
    "print(\"\\nStep 1: Loading and preparing the WikiArt dataset (all classes)...\")\n",
    "\n",
    "# Specify the path to the WikiArt dataset\n",
    "wikiart_dir = '../../scratch/mexas.v'  # Update this path as necessary\n",
    "\n",
    "# Get the list of class names (styles, artists, or genres)\n",
    "classes = [d for d in os.listdir(wikiart_dir) if os.path.isdir(os.path.join(wikiart_dir, d))]\n",
    "\n",
    "# Prepare a list to hold image paths and labels\n",
    "data = []\n",
    "\n",
    "for label in classes:\n",
    "    # Get all image paths for the current class\n",
    "    image_paths = glob.glob(os.path.join(wikiart_dir, label, '*.jpg'))\n",
    "    for path in image_paths:\n",
    "        data.append({\n",
    "            'image_path': path,\n",
    "            'label': label\n",
    "        })\n",
    "\n",
    "# Create a DataFrame\n",
    "image_df = pd.DataFrame(data)\n",
    "print(f\"Prepared image_df with {len(image_df)} entries from all classes.\")\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "image_df['encoded_label'] = label_encoder.fit_transform(image_df['label'])\n",
    "\n",
    "# Step 2: Define Image Transformations\n",
    "print(\"\\nStep 2: Defining image transformations...\")\n",
    "\n",
    "# Transformation for feature extraction (no augmentation)\n",
    "feature_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Transformation for obfuscating images (rotation, blur, etc.)\n",
    "# Note: Since we're applying transformations manually, no need for additional transforms here\n",
    "\n",
    "print(\"Image transformations defined.\")\n",
    "\n",
    "# Step 3: Create Custom Dataset and DataLoader\n",
    "print(\"\\nStep 3: Creating custom datasets and data loaders...\")\n",
    "\n",
    "class ArtImageDataset(Dataset):\n",
    "    def __init__(self, image_df, transform=None):\n",
    "        self.image_df = image_df.reset_index(drop=True)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_df.loc[idx, 'image_path']\n",
    "        label = self.image_df.loc[idx, 'encoded_label']\n",
    "        try:\n",
    "            img = Image.open(img_path).convert('RGB')\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image {img_path}: {e}\")\n",
    "            img = Image.new('RGB', (224, 224), (0, 0, 0))\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img, label  # Return image and label\n",
    "\n",
    "# Split the dataset into training, validation, and test sets\n",
    "print(\"\\nSplitting dataset into training, validation, and test sets...\")\n",
    "train_df, temp_df = train_test_split(\n",
    "    image_df, test_size=0.3, random_state=42, stratify=image_df['encoded_label']\n",
    ")\n",
    "val_df, test_df = train_test_split(\n",
    "    temp_df, test_size=0.5, random_state=42, stratify=temp_df['encoded_label']\n",
    ")\n",
    "print(f\"Training set: {len(train_df)} images\")\n",
    "print(f\"Validation set: {len(val_df)} images\")\n",
    "print(f\"Test set: {len(test_df)} images\")\n",
    "\n",
    "# Ensure there are no duplicates between the sets\n",
    "train_paths = set(train_df['image_path'])\n",
    "val_paths = set(val_df['image_path'])\n",
    "test_paths = set(test_df['image_path'])\n",
    "\n",
    "assert len(train_paths.intersection(val_paths)) == 0, \"Overlap between training and validation sets!\"\n",
    "assert len(train_paths.intersection(test_paths)) == 0, \"Overlap between training and test sets!\"\n",
    "assert len(val_paths.intersection(test_paths)) == 0, \"Overlap between validation and test sets!\"\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = ArtImageDataset(train_df, transform=feature_transform)\n",
    "val_dataset = ArtImageDataset(val_df, transform=feature_transform)\n",
    "test_dataset = ArtImageDataset(test_df, transform=feature_transform)\n",
    "\n",
    "# Create DataLoaders\n",
    "batch_size = 32  # Adjust based on your GPU memory\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=8, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=8, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=8, pin_memory=True)\n",
    "\n",
    "print(\"DataLoaders created for training, validation, and test sets.\")\n",
    "\n",
    "# Step 4: Set Up Device and Load Pre-trained Model\n",
    "print(\"\\nStep 4: Setting up the device and loading the pre-trained model...\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load the pre-trained ResNet-50 model\n",
    "model = models.resnet50(pretrained=True)\n",
    "\n",
    "# Modify the final layer for fine-tuning\n",
    "num_ftrs = model.fc.in_features\n",
    "num_classes = len(classes)\n",
    "model.fc = nn.Linear(num_ftrs, num_classes)\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "print(\"Pre-trained model loaded and modified for fine-tuning.\")\n",
    "\n",
    "# Step 5: Fine-Tune the Model\n",
    "print(\"\\nStep 5: Fine-tuning the model...\")\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "num_epochs = 4  # Adjust based on convergence\n",
    "best_val_acc = 0.0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "\n",
    "    for inputs, labels in tqdm(train_loader, desc=\"Training\"):\n",
    "        inputs = inputs.to(device, non_blocking=True)\n",
    "        labels = labels.to(device, non_blocking=True)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "    epoch_loss = running_loss / len(train_dataset)\n",
    "    epoch_acc = running_corrects.double() / len(train_dataset)\n",
    "\n",
    "    print(f\"Training Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}\")\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_running_loss = 0.0\n",
    "    val_running_corrects = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(val_loader, desc=\"Validation\"):\n",
    "            inputs = inputs.to(device, non_blocking=True)\n",
    "            labels = labels.to(device, non_blocking=True)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            val_running_loss += loss.item() * inputs.size(0)\n",
    "            val_running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "    val_epoch_loss = val_running_loss / len(val_dataset)\n",
    "    val_epoch_acc = val_running_corrects.double() / len(val_dataset)\n",
    "\n",
    "    print(f\"Validation Loss: {val_epoch_loss:.4f} Acc: {val_epoch_acc:.4f}\")\n",
    "\n",
    "    # Checkpointing\n",
    "    if val_epoch_acc > best_val_acc:\n",
    "        best_val_acc = val_epoch_acc\n",
    "        torch.save(model.state_dict(), 'best_resnet50_wikiart_finetuned.pth')\n",
    "        print(\"Best model updated.\")\n",
    "\n",
    "print(f\"\\nTraining complete. Best Validation Accuracy: {best_val_acc:.4f}\")\n",
    "\n",
    "# Load the best model\n",
    "model.load_state_dict(torch.load('best_resnet50_wikiart_finetuned.pth'))\n",
    "model.eval()\n",
    "print(\"Best fine-tuned model loaded.\")\n",
    "\n",
    "# Step 6: Extract Features with Conditional Loading\n",
    "print(\"\\nStep 6: Extracting features from training set...\")\n",
    "\n",
    "features_file = 'wikiart_features_finetuned.npy'\n",
    "\n",
    "if os.path.exists(features_file):\n",
    "    print(f\"Feature file '{features_file}' found. Loading existing features...\")\n",
    "    try:\n",
    "        features_np = np.load(features_file)\n",
    "        labels = np.load('wikiart_labels_finetuned.npy')\n",
    "        print(f\"Loaded features with shape: {features_np.shape}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading features from '{features_file}': {e}\")\n",
    "        print(\"Proceeding to extract features anew.\")\n",
    "\n",
    "        # Extract features as the file could not be loaded\n",
    "        features, labels = extract_features(model, train_loader, device)\n",
    "        print(f\"Extracted features shape: {features.shape}\")\n",
    "\n",
    "        # Convert features to numpy array\n",
    "        features_np = features.numpy().astype('float32')\n",
    "\n",
    "        # Save features and labels\n",
    "        np.save(features_file, features_np)\n",
    "        np.save('wikiart_labels_finetuned.npy', labels)\n",
    "        print(f\"Features extracted and saved to '{features_file}'.\")\n",
    "else:\n",
    "    print(f\"Feature file '{features_file}' not found. Extracting features...\")\n",
    "\n",
    "    # Extract features\n",
    "    features, labels = extract_features(model, train_loader, device)\n",
    "    print(f\"Extracted features shape: {features.shape}\")\n",
    "\n",
    "    # Convert features to numpy array\n",
    "    features_np = features.numpy().astype('float32')\n",
    "\n",
    "    # Save features and labels\n",
    "    np.save(features_file, features_np)\n",
    "    np.save('wikiart_labels_finetuned.npy', labels)\n",
    "    print(f\"Features extracted and saved to '{features_file}'.\")\n",
    "\n",
    "# Step 7: Create FAISS Index with Extracted Features\n",
    "print(\"\\nStep 7: Creating FAISS index with extracted features...\")\n",
    "\n",
    "index_file = 'wikiart_faiss_index_finetuned.bin'\n",
    "\n",
    "# Use FAISS with GPU support\n",
    "res = faiss.StandardGpuResources()  # Use a single GPU\n",
    "\n",
    "if os.path.exists(index_file):\n",
    "    print(f\"FAISS index file '{index_file}' found. Loading existing index...\")\n",
    "    try:\n",
    "        index_cpu = faiss.read_index(index_file)\n",
    "        index = faiss.index_cpu_to_gpu(res, 0, index_cpu)\n",
    "        print(f\"Loaded FAISS index from '{index_file}' and moved to GPU.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading FAISS index from '{index_file}': {e}\")\n",
    "        print(\"Proceeding to create a new FAISS index.\")\n",
    "\n",
    "        # Create FAISS index\n",
    "        index_cpu = faiss.IndexFlatIP(features_np.shape[1])\n",
    "        index_cpu.add(features_np)\n",
    "        index = faiss.index_cpu_to_gpu(res, 0, index_cpu)\n",
    "        faiss.write_index(index_cpu, index_file)\n",
    "        print(f\"New FAISS index created and saved to '{index_file}' and moved to GPU.\")\n",
    "else:\n",
    "    print(f\"FAISS index file '{index_file}' not found. Creating a new index...\")\n",
    "\n",
    "    # Create FAISS index\n",
    "    index_cpu = faiss.IndexFlatIP(features_np.shape[1])\n",
    "    index_cpu.add(features_np)\n",
    "    index = faiss.index_cpu_to_gpu(res, 0, index_cpu)\n",
    "    faiss.write_index(index_cpu, index_file)\n",
    "    print(f\"FAISS index created and saved to '{index_file}' and moved to GPU.\")\n",
    "\n",
    "# Step 8: Evaluate the Model on the Test Set\n",
    "print(\"\\nStep 8: Evaluating the model on the test set...\")\n",
    "\n",
    "test_features_file = 'wikiart_test_features_finetuned.npy'\n",
    "\n",
    "if os.path.exists(test_features_file):\n",
    "    print(f\"Feature file '{test_features_file}' found. Loading existing features...\")\n",
    "    try:\n",
    "        test_features_np = np.load(test_features_file)\n",
    "        test_labels = np.load('wikiart_test_labels_finetuned.npy')\n",
    "        print(f\"Loaded test features with shape: {test_features_np.shape}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading features from '{test_features_file}': {e}\")\n",
    "        print(\"Proceeding to extract test features anew.\")\n",
    "\n",
    "        # Extract features as the file could not be loaded\n",
    "        test_features, test_labels = extract_features(model, test_loader, device)\n",
    "        print(f\"Extracted test features shape: {test_features.shape}\")\n",
    "\n",
    "        # Convert features to numpy array\n",
    "        test_features_np = test_features.numpy().astype('float32')\n",
    "\n",
    "        # Save features and labels\n",
    "        np.save(test_features_file, test_features_np)\n",
    "        np.save('wikiart_test_labels_finetuned.npy', test_labels)\n",
    "        print(f\"Test features extracted and saved to '{test_features_file}'.\")\n",
    "else:\n",
    "    print(f\"Feature file '{test_features_file}' not found. Extracting test features...\")\n",
    "\n",
    "    # Extract test features\n",
    "    test_features, test_labels = extract_features(model, test_loader, device)\n",
    "    print(f\"Extracted test features shape: {test_features.shape}\")\n",
    "\n",
    "    # Convert features to numpy array\n",
    "    test_features_np = test_features.numpy().astype('float32')\n",
    "\n",
    "    # Save features and labels\n",
    "    np.save(test_features_file, test_features_np)\n",
    "    np.save('wikiart_test_labels_finetuned.npy', test_labels)\n",
    "    print(f\"Test features extracted and saved to '{test_features_file}'.\")\n",
    "\n",
    "# Perform batch similarity search for all test images\n",
    "print(\"\\nPerforming similarity search on the test set...\")\n",
    "\n",
    "k = 5  # Number of top matches to retrieve\n",
    "\n",
    "total_queries = len(test_features_np)\n",
    "batch_size_eval = 64  # Adjust based on your GPU memory\n",
    "num_batches = (total_queries + batch_size_eval - 1) // batch_size_eval\n",
    "\n",
    "correct_at_1 = 0\n",
    "correct_at_k = 0\n",
    "\n",
    "for batch_idx in tqdm(range(num_batches), desc=\"Evaluating\"):\n",
    "    start_idx = batch_idx * batch_size_eval\n",
    "    end_idx = min(start_idx + batch_size_eval, total_queries)\n",
    "    query_features_batch = test_features_np[start_idx:end_idx]\n",
    "    true_labels_batch = test_labels[start_idx:end_idx]\n",
    "\n",
    "    distances, indices = index.search(query_features_batch, k)\n",
    "\n",
    "    # Retrieve labels for the indices\n",
    "    retrieved_labels_batch = labels[indices]  # labels correspond to train features\n",
    "\n",
    "    for i in range(end_idx - start_idx):\n",
    "        true_label = true_labels_batch[i]\n",
    "        retrieved_labels = retrieved_labels_batch[i]\n",
    "\n",
    "        if retrieved_labels[0] == true_label:\n",
    "            correct_at_1 += 1\n",
    "\n",
    "        if true_label in retrieved_labels:\n",
    "            correct_at_k += 1\n",
    "\n",
    "top1_accuracy = correct_at_1 / total_queries * 100\n",
    "topk_accuracy = correct_at_k / total_queries * 100\n",
    "\n",
    "print(f\"\\nEvaluation Results:\")\n",
    "print(f\"Total Queries: {total_queries}\")\n",
    "print(f\"Top-1 Accuracy: {top1_accuracy:.2f}%\")\n",
    "print(f\"Top-{k} Accuracy: {topk_accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b475acb8-aa90-4fbd-938a-c20c91a0b81e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FINAL",
   "language": "python",
   "name": "final"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
