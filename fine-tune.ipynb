{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "868d9a72-003b-4f79-96d4-3cd03a9015c3",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0xdf in position 3: invalid continuation byte",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_21188/2602989601.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m  \u001b[0;31m# For progress bars\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/FINAL/lib/python3.7/site-packages/torch/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    188\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mUSE_GLOBAL_DEPS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0m_load_global_deps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[0;31m# Appease the type checker; ordinarily this binding is inserted by the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0xdf in position 3: invalid continuation byte"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm  # For progress bars\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import models, transforms\n",
    "import torch.nn as nn\n",
    "\n",
    "import faiss\n",
    "\n",
    "# Additional imports for fine-tuning\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Ensure inline plotting for Jupyter notebooks (if using a notebook)\n",
    "# %matplotlib inline\n",
    "\n",
    "# Step 0: Define Helper Functions\n",
    "print(\"Step 0: Defining helper functions...\")\n",
    "\n",
    "def download_images(final_df, save_dir='./art_images/'):\n",
    "    \"\"\"\n",
    "    Downloads images from URLs specified in the final_df DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    - final_df (pd.DataFrame): DataFrame containing 'objectid' and 'image_urls' columns.\n",
    "    - save_dir (str): Directory where images will be saved.\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "        print(f\"Created directory: {save_dir}\")\n",
    "    else:\n",
    "        print(f\"Directory already exists: {save_dir}\")\n",
    "\n",
    "    print(\"Starting image downloads...\")\n",
    "    for idx, row in tqdm(final_df.iterrows(), total=final_df.shape[0], desc=\"Downloading images\"):\n",
    "        objectid = str(row['objectid'])\n",
    "        image_urls = row['image_urls']\n",
    "        for i, image_url in enumerate(image_urls):\n",
    "            image_filename = f\"{objectid}_{i}.jpg\"\n",
    "            image_path = os.path.join(save_dir, image_filename)\n",
    "\n",
    "            # Skip downloading if the file already exists\n",
    "            if os.path.exists(image_path):\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                response = requests.get(image_url, timeout=10)\n",
    "                response.raise_for_status()  # Check if the request was successful\n",
    "                with open(image_path, 'wb') as f:\n",
    "                    f.write(response.content)\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                print(f\"Failed to download {image_filename} from {image_url}: {e}\")\n",
    "\n",
    "def show_results(image_path_or_url, results):\n",
    "    \"\"\"\n",
    "    Displays the query image alongside its top matching results.\n",
    "\n",
    "    Parameters:\n",
    "    - image_path_or_url (str): Path or URL of the query image.\n",
    "    - results (pd.DataFrame): DataFrame containing the top matching artworks.\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    print(\"\\nDisplaying results visually...\")\n",
    "    try:\n",
    "        if image_path_or_url.startswith('http'):\n",
    "            response = requests.get(image_path_or_url, timeout=10)\n",
    "            query_img = Image.open(BytesIO(response.content)).convert('RGB')\n",
    "        else:\n",
    "            query_img = Image.open(image_path_or_url).convert('RGB')\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load query image: {e}\")\n",
    "        return\n",
    "\n",
    "    num_results = min(len(results), 5)\n",
    "    plt.figure(figsize=(5 * (num_results + 1), 5))\n",
    "\n",
    "    # Display Query Image\n",
    "    plt.subplot(1, num_results + 1, 1)\n",
    "    plt.imshow(query_img)\n",
    "    plt.title('Query Image')\n",
    "    plt.axis('off')\n",
    "\n",
    "    # Display Matching Images\n",
    "    for i in range(num_results):\n",
    "        img_path = results.iloc[i]['image_path']\n",
    "        try:\n",
    "            img = Image.open(img_path).convert('RGB')\n",
    "            plt.subplot(1, num_results + 1, i + 2)\n",
    "            plt.imshow(img)\n",
    "            plt.title(f\"Match {i+1}\")\n",
    "            plt.axis('off')\n",
    "            print(f\"Loaded Match {i+1}: {img_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to load image {img_path}: {e}\")\n",
    "\n",
    "    plt.show()\n",
    "    print(\"Results displayed.\")\n",
    "\n",
    "def process_input_image(image_path_or_url, transform, device):\n",
    "    \"\"\"\n",
    "    Processes the input image by loading, transforming, and sending it to the device.\n",
    "\n",
    "    Parameters:\n",
    "    - image_path_or_url (str): Path or URL of the image.\n",
    "    - transform (torchvision.transforms.Compose): Transformations to apply.\n",
    "    - device (torch.device): Device to send the image tensor.\n",
    "\n",
    "    Returns:\n",
    "    - torch.Tensor: Processed image tensor.\n",
    "    \"\"\"\n",
    "    print(f\"  Processing input image: {image_path_or_url}\")\n",
    "    try:\n",
    "        if image_path_or_url.startswith('http'):\n",
    "            response = requests.get(image_path_or_url, timeout=10)\n",
    "            img = Image.open(BytesIO(response.content)).convert('RGB')\n",
    "        else:\n",
    "            img = Image.open(image_path_or_url).convert('RGB')\n",
    "    except Exception as e:\n",
    "        print(f\"  Failed to load image {image_path_or_url}: {e}\")\n",
    "        # Return a black image in case of error\n",
    "        img = Image.new('RGB', (224, 224), (0, 0, 0))\n",
    "    img = transform(img).unsqueeze(0).to(device)\n",
    "    print(\"  Image processed and transformed.\")\n",
    "    return img\n",
    "\n",
    "def get_image_features(model, img_tensor):\n",
    "    \"\"\"\n",
    "    Extracts and normalizes features from the image tensor.\n",
    "\n",
    "    Parameters:\n",
    "    - model (torch.nn.Module): The feature extraction model.\n",
    "    - img_tensor (torch.Tensor): Image tensor.\n",
    "\n",
    "    Returns:\n",
    "    - np.ndarray: Normalized feature vector.\n",
    "    \"\"\"\n",
    "    print(\"  Extracting features from the query image...\")\n",
    "    with torch.no_grad():\n",
    "        features = model(img_tensor)\n",
    "        features = features.view(features.size(0), -1)  # Flatten\n",
    "        features = features / features.norm()\n",
    "    print(\"  Features extracted and normalized.\")\n",
    "    return features.cpu().numpy().astype('float32')\n",
    "\n",
    "def search_artwork(query_features, index, k=5):\n",
    "    \"\"\"\n",
    "    Searches for the top k similar artworks using FAISS.\n",
    "\n",
    "    Parameters:\n",
    "    - query_features (np.ndarray): Feature vector of the query image.\n",
    "    - index (faiss.Index): FAISS index.\n",
    "    - k (int): Number of top matches to retrieve.\n",
    "\n",
    "    Returns:\n",
    "    - distances (np.ndarray): Similarity scores.\n",
    "    - indices (np.ndarray): Indices of the top matches.\n",
    "    \"\"\"\n",
    "    print(f\"  Searching for top {k} similar artworks...\")\n",
    "    distances, indices = index.search(query_features, k)\n",
    "    print(\"  Search completed.\")\n",
    "    return distances[0], indices[0]\n",
    "\n",
    "def get_artwork_info(indices, image_df):\n",
    "    \"\"\"\n",
    "    Retrieves artwork information based on the indices.\n",
    "\n",
    "    Parameters:\n",
    "    - indices (np.ndarray): Indices of the top matches.\n",
    "    - image_df (pd.DataFrame): DataFrame containing image metadata.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: DataFrame of the top matching artworks.\n",
    "    \"\"\"\n",
    "    print(\"  Retrieving artwork information for the top matches...\")\n",
    "    try:\n",
    "        results = image_df.iloc[indices].reset_index(drop=True)\n",
    "        print(\"  Artwork information retrieved.\")\n",
    "    except Exception as e:\n",
    "        print(f\"  Error retrieving artwork information: {e}\")\n",
    "        results = pd.DataFrame()  # Return empty DataFrame on error\n",
    "    return results\n",
    "\n",
    "def identify_artwork(image_path_or_url, model, index, image_df, transform, device, k=5):\n",
    "    \"\"\"\n",
    "    Identifies the top k artworks similar to the query image.\n",
    "\n",
    "    Parameters:\n",
    "    - image_path_or_url (str): Path or URL of the query image.\n",
    "    - model (torch.nn.Module): Feature extraction model.\n",
    "    - index (faiss.Index): FAISS index.\n",
    "    - image_df (pd.DataFrame): DataFrame containing image metadata.\n",
    "    - transform (torchvision.transforms.Compose): Transformations to apply.\n",
    "    - device (torch.device): Device to send the image tensor.\n",
    "    - k (int): Number of top matches to retrieve.\n",
    "\n",
    "    Returns:\n",
    "    - results (pd.DataFrame): DataFrame of the top matching artworks.\n",
    "    - distances (np.ndarray): Similarity scores.\n",
    "    \"\"\"\n",
    "    print(\"\\nIdentifying artwork...\")\n",
    "    img_tensor = process_input_image(image_path_or_url, transform, device)\n",
    "    query_features = get_image_features(model, img_tensor)\n",
    "    distances, indices = search_artwork(query_features, index, k)\n",
    "    results = get_artwork_info(indices, image_df)\n",
    "    print(\"Artwork identification completed.\")\n",
    "    return results, distances\n",
    "\n",
    "def extract_features(model, dataloader, device):\n",
    "    \"\"\"\n",
    "    Extracts features from images using the provided model and dataloader.\n",
    "\n",
    "    Parameters:\n",
    "    - model (torch.nn.Module): The feature extraction model.\n",
    "    - dataloader (DataLoader): DataLoader for the dataset.\n",
    "    - device (torch.device): Device to perform computations on.\n",
    "\n",
    "    Returns:\n",
    "    - torch.Tensor: Extracted features.\n",
    "    \"\"\"\n",
    "    print(\"Starting feature extraction...\")\n",
    "    features = []\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (imgs, idxs) in enumerate(tqdm(dataloader, desc=\"Extracting features\"), 1):\n",
    "            imgs = imgs.to(device)\n",
    "            try:\n",
    "                outputs = model(imgs)\n",
    "                outputs = outputs.view(outputs.size(0), -1)  # Flatten to (batch_size, feature_dim)\n",
    "                features.append(outputs.cpu())\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing batch {batch_idx}: {e}\")\n",
    "    features = torch.cat(features, dim=0)\n",
    "    print(\"Feature extraction completed.\")\n",
    "    return features\n",
    "\n",
    "# Step 1: Load and Prepare Datasets\n",
    "print(\"\\nStep 1: Loading and preparing datasets...\")\n",
    "\n",
    "try:\n",
    "    objects_df = pd.read_csv('./datasets/objects.csv')\n",
    "    constituents_df = pd.read_csv('./datasets/constituents.csv')\n",
    "    images_df = pd.read_csv('./datasets/published_images.csv')\n",
    "    print(\"Datasets loaded successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading datasets: {e}\")\n",
    "    raise\n",
    "\n",
    "# Ensure relevant columns are strings and stripped\n",
    "for df, key in [\n",
    "    (objects_df, 'objectid'),\n",
    "    (constituents_df, 'artistofngaobject'),\n",
    "    (images_df, 'depictstmsobjectid')\n",
    "]:\n",
    "    df[key] = df[key].astype(str).str.strip()\n",
    "\n",
    "# Aggregate artists\n",
    "print(\"Aggregating artist information...\")\n",
    "artists_agg = constituents_df.groupby('artistofngaobject')['preferreddisplayname'] \\\n",
    "    .apply(lambda names: ', '.join(sorted(set(names)))).reset_index()\n",
    "artists_agg.rename(columns={\n",
    "    'artistofngaobject': 'objectid',\n",
    "    'preferreddisplayname': 'artists'\n",
    "}, inplace=True)\n",
    "\n",
    "# Aggregate image URLs\n",
    "print(\"Aggregating image URLs...\")\n",
    "images_agg = images_df.groupby('depictstmsobjectid')['iiifthumburl'] \\\n",
    "    .apply(list).reset_index()\n",
    "images_agg.rename(columns={\n",
    "    'depictstmsobjectid': 'objectid',\n",
    "    'iiifthumburl': 'image_urls'\n",
    "}, inplace=True)\n",
    "\n",
    "# Merge datasets\n",
    "print(\"Merging datasets into final_df...\")\n",
    "merged_with_artists = objects_df.merge(\n",
    "    artists_agg,\n",
    "    on='objectid',\n",
    "    how='left'\n",
    ")\n",
    "final_df = merged_with_artists.merge(\n",
    "    images_agg,\n",
    "    on='objectid',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Select and clean relevant columns\n",
    "final_df = final_df[['objectid', 'title', 'artists', 'image_urls']]\n",
    "final_df['artists'] = final_df['artists'].fillna('Unknown Artist')\n",
    "final_df['image_urls'] = final_df['image_urls'].apply(lambda x: x if isinstance(x, list) else [])\n",
    "final_df = final_df[final_df['image_urls'].map(len) > 0].reset_index(drop=True)\n",
    "\n",
    "# Print summary statistics\n",
    "print(\"\\nSummary of final_df:\")\n",
    "print(f\"Unique artworks in objects_df: {objects_df['objectid'].nunique()}\")\n",
    "print(f\"Unique artists in constituents_df: {constituents_df['constituentid'].nunique()}\")\n",
    "print(f\"Unique image URLs in images_df: {images_df['iiifthumburl'].nunique()}\")\n",
    "print(f\"Unique artworks in final_df: {final_df['objectid'].nunique()}\")\n",
    "print(f\"Columns in final_df: {final_df.columns.tolist()}\")\n",
    "\n",
    "# Step 2: Download Images (Optional)\n",
    "# Uncomment the following lines if you need to download images\n",
    "# print(\"\\nStep 2: Downloading images...\")\n",
    "# download_images(final_df)\n",
    "\n",
    "print(\"\\nStep 3: Preparing image_df from downloaded images...\")\n",
    "\n",
    "# Check if 'image_df.pkl' already exists\n",
    "if os.path.exists('image_df.pkl'):\n",
    "    # Load image_df from the pickle file\n",
    "    image_df = pd.read_pickle('image_df.pkl')\n",
    "    print(f\"Loaded image_df with {len(image_df)} entries from 'image_df.pkl'.\")\n",
    "else:\n",
    "    image_paths = glob.glob('./art_images/*.jpg')\n",
    "    print(f\"Found {len(image_paths)} images in './art_images/' directory.\")\n",
    "    \n",
    "    # Use all images\n",
    "    subset_image_paths = image_paths\n",
    "    print(f\"Selected {len(subset_image_paths)} images for processing.\")\n",
    "    \n",
    "    # Prepare image_data\n",
    "    image_data = []\n",
    "    for path in tqdm(subset_image_paths, desc=\"Preparing image data\"):\n",
    "        filename = os.path.basename(path)\n",
    "        objectid = filename.split('_')[0]\n",
    "        row = final_df[final_df['objectid'] == objectid]\n",
    "        if row.empty:\n",
    "            # Skip images without metadata\n",
    "            continue\n",
    "        row = row.iloc[0]\n",
    "        image_data.append({\n",
    "            'image_path': path,\n",
    "            'objectid': objectid,\n",
    "            'artists': row['artists'],\n",
    "            'title': row['title']\n",
    "        })\n",
    "    \n",
    "    # Create image_df\n",
    "    image_df = pd.DataFrame(image_data)\n",
    "    print(f\"Prepared image_df with {len(image_df)} entries.\")\n",
    "    \n",
    "    # Save image_df to a pickle file for future use\n",
    "    image_df.to_pickle('image_df.pkl')\n",
    "    print(\"Saved image_df to 'image_df.pkl' for future use.\")\n",
    "\n",
    "# Step 4: Preparing Dataset for Fine-Tuning\n",
    "print(\"\\nStep 4: Preparing dataset for fine-tuning...\")\n",
    "\n",
    "# Analyze the number of samples per title and artist\n",
    "title_counts = image_df['title'].value_counts()\n",
    "artist_counts = image_df['artists'].value_counts()\n",
    "\n",
    "print(f\"Number of unique titles: {len(title_counts)}\")\n",
    "print(f\"Number of unique artists: {len(artist_counts)}\")\n",
    "\n",
    "# Decide whether to classify by title or artist based on counts\n",
    "min_samples_per_class = 2\n",
    "\n",
    "# Check if titles have enough samples\n",
    "titles_with_enough_samples = title_counts[title_counts >= min_samples_per_class]\n",
    "if len(titles_with_enough_samples) > 0:\n",
    "    print(f\"Proceeding with classification by title using {len(titles_with_enough_samples)} titles.\")\n",
    "    # Filter image_df\n",
    "    image_df = image_df[image_df['title'].isin(titles_with_enough_samples.index)]\n",
    "    # Encode labels\n",
    "    label_encoder = LabelEncoder()\n",
    "    image_df['label'] = label_encoder.fit_transform(image_df['title'])\n",
    "else:\n",
    "    print(\"Not enough samples per title. Proceeding with classification by artist.\")\n",
    "    # Filter artists with enough samples\n",
    "    artists_with_enough_samples = artist_counts[artist_counts >= min_samples_per_class]\n",
    "    image_df = image_df[image_df['artists'].isin(artists_with_enough_samples.index)]\n",
    "    # Encode labels\n",
    "    label_encoder = LabelEncoder()\n",
    "    image_df['label'] = label_encoder.fit_transform(image_df['artists'])\n",
    "\n",
    "print(f\"Filtered image_df now has {len(image_df)} entries.\")\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "print(\"\\nSplitting dataset into training and validation sets...\")\n",
    "train_df, val_df = train_test_split(\n",
    "    image_df,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    # stratify=image_df['label']\n",
    ")\n",
    "\n",
    "print(f\"Training set size: {len(train_df)}\")\n",
    "print(f\"Validation set size: {len(val_df)}\")\n",
    "\n",
    "# Step 5: Define Image Transformations with Data Augmentation\n",
    "print(\"\\nStep 5: Defining image transformations with data augmentation...\")\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "print(\"Image transformations defined.\")\n",
    "\n",
    "# Step 6: Create Custom Dataset and DataLoaders\n",
    "print(\"\\nStep 6: Creating custom dataset and data loaders...\")\n",
    "\n",
    "class ArtImageDataset(Dataset):\n",
    "    def __init__(self, image_df, transform=None):\n",
    "        self.image_df = image_df.reset_index(drop=True)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_df.loc[idx, 'image_path']\n",
    "        label = self.image_df.loc[idx, 'label']\n",
    "\n",
    "        try:\n",
    "            img = Image.open(img_path).convert('RGB')\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image {img_path}: {e}\")\n",
    "            img = Image.new('RGB', (224, 224), (0, 0, 0))\n",
    "\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return img, label\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = ArtImageDataset(train_df, transform=train_transform)\n",
    "val_dataset = ArtImageDataset(val_df, transform=val_transform)\n",
    "\n",
    "# Create DataLoaders\n",
    "batch_size = 32  # Adjust based on your GPU memory\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=12)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=12)\n",
    "\n",
    "print(f\"Training DataLoader created with {len(train_loader)} batches.\")\n",
    "print(f\"Validation DataLoader created with {len(val_loader)} batches.\")\n",
    "\n",
    "# Step 7: Set Up Device and Modify the Model\n",
    "print(\"\\nStep 7: Setting up the device and modifying the ResNet-50 model for fine-tuning...\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Get the number of classes\n",
    "num_classes = image_df['label'].nunique()\n",
    "print(f\"Number of classes: {num_classes}\")\n",
    "\n",
    "# Load the pre-trained ResNet-50 model\n",
    "model_ft = models.resnet50(pretrained=True)\n",
    "\n",
    "# Optionally freeze initial layers\n",
    "for param in model_ft.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Replace the final fully connected layer\n",
    "num_ftrs = model_ft.fc.in_features\n",
    "model_ft.fc = nn.Linear(num_ftrs, num_classes)\n",
    "\n",
    "# Move the model to the device\n",
    "model_ft = model_ft.to(device)\n",
    "print(\"Model modified for fine-tuning.\")\n",
    "\n",
    "# Step 8: Define Loss Function and Optimizer\n",
    "print(\"\\nStep 8: Defining loss function and optimizer...\")\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Only parameters of the final layer are being optimized\n",
    "optimizer = torch.optim.Adam(model_ft.fc.parameters(), lr=0.001)\n",
    "\n",
    "# Optionally, use a learning rate scheduler\n",
    "from torch.optim import lr_scheduler\n",
    "scheduler = lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n",
    "\n",
    "print(\"Loss function and optimizer defined.\")\n",
    "\n",
    "num_epochs = 5  # Adjust based on your needs\n",
    "\n",
    "# Step 9: Implement the Training Loop\n",
    "print(\"\\nStep 9: Starting the training loop...\")\n",
    "\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "val_losses = []\n",
    "val_accuracies = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "    print(\"-\" * 10)\n",
    "\n",
    "    # Training phase\n",
    "    model_ft.train()\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "\n",
    "    for inputs, labels in tqdm(train_loader, desc=\"Training\"):\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model_ft(inputs)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Statistics\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "    scheduler.step()  # Step the learning rate scheduler\n",
    "\n",
    "    epoch_loss = running_loss / len(train_dataset)\n",
    "    epoch_acc = running_corrects.double() / len(train_dataset)\n",
    "\n",
    "    train_losses.append(epoch_loss)\n",
    "    train_accuracies.append(epoch_acc.item())\n",
    "\n",
    "    print(f\"Training Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}\")\n",
    "\n",
    "    # Validation phase\n",
    "    model_ft.eval()\n",
    "    val_running_loss = 0.0\n",
    "    val_running_corrects = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(val_loader, desc=\"Validation\"):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model_ft(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            val_running_loss += loss.item() * inputs.size(0)\n",
    "            val_running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "    val_epoch_loss = val_running_loss / len(val_dataset)\n",
    "    val_epoch_acc = val_running_corrects.double() / len(val_dataset)\n",
    "\n",
    "    val_losses.append(val_epoch_loss)\n",
    "    val_accuracies.append(val_epoch_acc.item())\n",
    "\n",
    "    print(f\"Validation Loss: {val_epoch_loss:.4f} Acc: {val_epoch_acc:.4f}\")\n",
    "\n",
    "# After training, create plots and save them\n",
    "print(\"\\nStep 9.1: Plotting and saving training curves...\")\n",
    "\n",
    "# Ensure the 'training_images' directory exists\n",
    "if not os.path.exists('training_images'):\n",
    "    os.makedirs('training_images')\n",
    "    print(\"Created directory 'training_images'.\")\n",
    "\n",
    "# Plot Training and Validation Loss\n",
    "epochs_range = range(1, num_epochs + 1)\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(epochs_range, train_losses, 'b-', label='Training Loss')\n",
    "plt.plot(epochs_range, val_losses, 'r-', label='Validation Loss')\n",
    "plt.title(f'Training and Validation Loss\\nModel: ResNet50, Epochs: {num_epochs}')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig('training_images/loss_plot.png')\n",
    "plt.close()\n",
    "print(\"Saved loss plot to 'training_images/loss_plot.png'.\")\n",
    "\n",
    "# Plot Training and Validation Accuracy\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(epochs_range, train_accuracies, 'b-', label='Training Accuracy')\n",
    "plt.plot(epochs_range, val_accuracies, 'r-', label='Validation Accuracy')\n",
    "plt.title(f'Training and Validation Accuracy\\nModel: ResNet50, Epochs: {num_epochs}')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig('training_images/accuracy_plot.png')\n",
    "plt.close()\n",
    "print(\"Saved accuracy plot to 'training_images/accuracy_plot.png'.\")\n",
    "\n",
    "# Step 10: Save the Fine-Tuned Model\n",
    "print(\"\\nStep 10: Saving the fine-tuned model...\")\n",
    "    \n",
    "torch.save(model_ft.state_dict(), 'fine_tuned_resnet50_artworks.pth')\n",
    "print(\"Fine-tuned model saved to 'fine_tuned_resnet50_artworks.pth'.\")\n",
    "\n",
    "# Step 11: Load Fine-Tuned Model and Extract Features\n",
    "print(\"\\nStep 11: Loading fine-tuned model and extracting features...\")\n",
    "\n",
    "# Load the fine-tuned model weights\n",
    "model_ft.load_state_dict(torch.load('fine_tuned_resnet50_artworks.pth'))\n",
    "print(\"Loaded fine-tuned model weights.\")\n",
    "\n",
    "# Remove the classification layer for feature extraction\n",
    "model_extractor = nn.Sequential(*list(model_ft.children())[:-1])\n",
    "model_extractor = model_extractor.to(device)\n",
    "model_extractor.eval()\n",
    "\n",
    "# Use the same DataLoader without data augmentation for feature extraction\n",
    "feature_dataset = ArtImageDataset(image_df, transform=val_transform)\n",
    "feature_loader = DataLoader(feature_dataset, batch_size=batch_size, shuffle=False, num_workers=12)\n",
    "\n",
    "# Extract features\n",
    "art_features = extract_features(model_extractor, feature_loader, device)\n",
    "print(f\"Extracted features shape: {art_features.shape}\")\n",
    "\n",
    "# Normalize features\n",
    "art_features_norm = art_features / art_features.norm(dim=1, keepdim=True)\n",
    "art_features_np = art_features_norm.numpy().astype('float32')\n",
    "\n",
    "# Save features\n",
    "np.save('fine_tuned_art_features.npy', art_features_np)\n",
    "print(\"Features from fine-tuned model saved to 'fine_tuned_art_features.npy'.\")\n",
    "\n",
    "# Step 12: Create FAISS Index and Add Features\n",
    "print(\"\\nStep 12: Creating FAISS index with fine-tuned features...\")\n",
    "\n",
    "index = faiss.IndexFlatIP(art_features_np.shape[1])\n",
    "index.add(art_features_np)\n",
    "faiss.write_index(index, 'fine_tuned_faiss_index.bin')\n",
    "print(\"FAISS index created with fine-tuned features and saved to 'fine_tuned_faiss_index.bin'.\")\n",
    "\n",
    "# Step 13: Perform the Query on a Sample Image\n",
    "print(\"\\nStep 13: Performing query on a sample image...\")\n",
    "\n",
    "# Use any image from the dataset as the query\n",
    "query_image_index = 80  # Adjust index as needed\n",
    "if query_image_index >= len(image_df):\n",
    "    query_image_index = len(image_df) - 1  # Ensure the index is within range\n",
    "image_to_query = image_df.iloc[query_image_index]['image_path']\n",
    "print(f\"Selected image for querying: {image_to_query}\")\n",
    "\n",
    "try:\n",
    "    # Use the fine-tuned feature extractor and FAISS index\n",
    "    results, distances = identify_artwork(\n",
    "        image_to_query,\n",
    "        model_extractor,\n",
    "        index,\n",
    "        image_df,\n",
    "        val_transform,\n",
    "        device,\n",
    "        k=5\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f\"Error identifying artwork: {e}\")\n",
    "    results, distances = None, None\n",
    "\n",
    "# Step 14: Display Top Matching Artworks\n",
    "if results is not None and not results.empty:\n",
    "    print(\"\\nTop matching artworks:\")\n",
    "    for i, (idx, dist) in enumerate(zip(results.index, distances)):\n",
    "        row = results.iloc[i]\n",
    "        print(f\"Rank {i+1}:\")\n",
    "        print(f\"  Artwork Title: {row['title']}\")\n",
    "        print(f\"  Artist: {row['artists']}\")\n",
    "        print(f\"  Similarity Score: {dist:.4f}\")\n",
    "        print(f\"  Image Path: {row['image_path']}\\n\")\n",
    "else:\n",
    "    print(\"No results to display due to an error in identifying the artwork.\")\n",
    "\n",
    "# Step 15: Display Results Visually\n",
    "if results is not None and not results.empty:\n",
    "    print(\"\\nStep 15: Displaying results visually...\")\n",
    "    show_results(image_to_query, results)\n",
    "else:\n",
    "    print(\"Skipping visual display due to earlier errors.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ae78efb-6b20-4c6b-a43e-8ce890a6fc87",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_21188/3816279948.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Is CUDA available?:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Number of GPUs available:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"GPU Device Name:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_device_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Is CUDA available?:\", torch.cuda.is_available())\n",
    "print(\"Number of GPUs available:\", torch.cuda.device_count())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU Device Name:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print(\"CUDA is not available.\")\n",
    "print(\"PyTorch Version:\", torch.__version__)\n",
    "print(\"CUDA Version used by PyTorch:\", torch.version.cuda)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a0a9087-e98b-4b53-9406-c60dfee3e829",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e54b7b4-796d-4ec2-9ec7-830d920d2cf5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998c2042-6c97-4dd5-9078-da00dbc55adc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FINAL",
   "language": "python",
   "name": "final"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
