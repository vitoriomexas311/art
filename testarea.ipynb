{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "79a64046-fd07-4d2e-904c-05e165a8c56d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0: Defining helper functions...\n",
      "\n",
      "Step 1: Loading and preparing the WikiArt dataset (all classes)...\n",
      "Prepared image_df with 200111 entries from all classes.\n",
      "\n",
      "Splitting dataset into training, validation, and test sets...\n",
      "Training set: 140077 images\n",
      "Validation set: 30017 images\n",
      "Test set: 30017 images\n",
      "\n",
      "Step 2: Defining image transformations...\n",
      "Image transformations defined.\n",
      "\n",
      "Step 3: Creating custom datasets and data loaders...\n",
      "DataLoaders created for training, validation, and test sets.\n",
      "\n",
      "Step 4: Setting up the device and loading the pre-trained model...\n",
      "Using device: cuda\n",
      "Pre-trained model loaded and modified for fine-tuning.\n",
      "\n",
      "Step 5: Fine-tuning the model...\n",
      "\n",
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 1/140077 [00:01<58:13:07,  1.50s/it]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 2.00 MiB (GPU 0; 11.17 GiB total capacity; 5.94 GiB already allocated; 128.00 KiB free; 6.06 GiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_22229/1527339706.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    371\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    372\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 373\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    374\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/FINAL/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/FINAL/lib/python3.7/site-packages/torchvision/models/resnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 220\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/FINAL/lib/python3.7/site-packages/torchvision/models/resnet.py\u001b[0m in \u001b[0;36m_forward_impl\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    208\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/FINAL/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/FINAL/lib/python3.7/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/FINAL/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/FINAL/lib/python3.7/site-packages/torchvision/models/resnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownsample\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/FINAL/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/FINAL/lib/python3.7/site-packages/torch/nn/modules/batchnorm.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    134\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunning_mean\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrack_running_stats\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunning_var\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrack_running_stats\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m             self.weight, self.bias, bn_training, exponential_average_factor, self.eps)\n\u001b[0m\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/FINAL/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   2056\u001b[0m     return torch.batch_norm(\n\u001b[1;32m   2057\u001b[0m         \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunning_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunning_var\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2058\u001b[0;31m         \u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcudnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2059\u001b[0m     )\n\u001b[1;32m   2060\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 2.00 MiB (GPU 0; 11.17 GiB total capacity; 5.94 GiB already allocated; 128.00 KiB free; 6.06 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')  # Use a non-interactive backend suitable for script running\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm  # For progress bars\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import models, transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import faiss\n",
    "\n",
    "# Additional imports for label encoding and model evaluation\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "\n",
    "# Step 0: Define Helper Functions\n",
    "print(\"Step 0: Defining helper functions...\")\n",
    "\n",
    "def show_results(image_path_or_url, results, query_index):\n",
    "    \"\"\"\n",
    "    Displays the query image alongside its top matching results and saves the figure.\n",
    "\n",
    "    Parameters:\n",
    "    - image_path_or_url (str): Path or URL of the query image.\n",
    "    - results (pd.DataFrame): DataFrame containing the top matching artworks.\n",
    "    - query_index (int): Index of the query image.\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    print(\"\\nDisplaying results visually...\")\n",
    "    try:\n",
    "        if image_path_or_url.startswith('http'):\n",
    "            response = requests.get(image_path_or_url, timeout=10)\n",
    "            query_img = Image.open(BytesIO(response.content)).convert('RGB')\n",
    "        else:\n",
    "            query_img = Image.open(image_path_or_url).convert('RGB')\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load query image: {e}\")\n",
    "        return\n",
    "\n",
    "    num_results = min(len(results), 5)\n",
    "    plt.figure(figsize=(5 * (num_results + 1), 5))\n",
    "\n",
    "    # Display Query Image\n",
    "    plt.subplot(1, num_results + 1, 1)\n",
    "    plt.imshow(query_img)\n",
    "    plt.title('Query Image')\n",
    "    plt.axis('off')\n",
    "\n",
    "    # Display Matching Images\n",
    "    for i in range(num_results):\n",
    "        img_path = results.iloc[i]['image_path']\n",
    "        try:\n",
    "            img = Image.open(img_path).convert('RGB')\n",
    "            plt.subplot(1, num_results + 1, i + 2)\n",
    "            plt.imshow(img)\n",
    "            plt.title(f\"Match {i+1}\")\n",
    "            plt.axis('off')\n",
    "            print(f\"Loaded Match {i+1}: {img_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to load image {img_path}: {e}\")\n",
    "\n",
    "    output_dir = 'results'\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    output_path = os.path.join(output_dir, f'query_results_{query_index}.png')\n",
    "    plt.savefig(output_path)\n",
    "    plt.close()\n",
    "    print(f\"Results displayed and saved to '{output_path}'.\")\n",
    "\n",
    "def process_input_image(image_path_or_url, transform, device):\n",
    "    \"\"\"\n",
    "    Processes the input image by loading, transforming, and sending it to the device.\n",
    "\n",
    "    Parameters:\n",
    "    - image_path_or_url (str): Path or URL of the image.\n",
    "    - transform (torchvision.transforms.Compose): Transformations to apply.\n",
    "    - device (torch.device): Device to send the image tensor.\n",
    "\n",
    "    Returns:\n",
    "    - torch.Tensor: Processed image tensor.\n",
    "    \"\"\"\n",
    "    print(f\"  Processing input image: {image_path_or_url}\")\n",
    "    try:\n",
    "        if image_path_or_url.startswith('http'):\n",
    "            response = requests.get(image_path_or_url, timeout=10)\n",
    "            img = Image.open(BytesIO(response.content)).convert('RGB')\n",
    "        else:\n",
    "            img = Image.open(image_path_or_url).convert('RGB')\n",
    "    except Exception as e:\n",
    "        print(f\"  Failed to load image {image_path_or_url}: {e}\")\n",
    "        # Return a black image in case of error\n",
    "        img = Image.new('RGB', (224, 224), (0, 0, 0))\n",
    "    img = transform(img).unsqueeze(0).to(device)\n",
    "    print(\"  Image processed and transformed.\")\n",
    "    return img\n",
    "\n",
    "def get_image_features(model, img_tensor):\n",
    "    \"\"\"\n",
    "    Extracts and normalizes features from the image tensor.\n",
    "\n",
    "    Parameters:\n",
    "    - model (torch.nn.Module): The feature extraction model.\n",
    "    - img_tensor (torch.Tensor): Image tensor.\n",
    "\n",
    "    Returns:\n",
    "    - np.ndarray: Normalized feature vector.\n",
    "    \"\"\"\n",
    "    print(\"  Extracting features from the query image...\")\n",
    "    with torch.no_grad():\n",
    "        features = model(img_tensor)\n",
    "        features = features.view(features.size(0), -1)  # Flatten\n",
    "        features = features / features.norm(dim=1, keepdim=True)\n",
    "    print(\"  Features extracted and normalized.\")\n",
    "    return features.cpu().numpy().astype('float32')\n",
    "\n",
    "def search_artwork(query_features, index, k=5):\n",
    "    \"\"\"\n",
    "    Searches for the top k similar artworks using FAISS.\n",
    "\n",
    "    Parameters:\n",
    "    - query_features (np.ndarray): Feature vector of the query image.\n",
    "    - index (faiss.Index): FAISS index.\n",
    "    - k (int): Number of top matches to retrieve.\n",
    "\n",
    "    Returns:\n",
    "    - distances (np.ndarray): Similarity scores.\n",
    "    - indices (np.ndarray): Indices of the top matches.\n",
    "    \"\"\"\n",
    "    print(f\"  Searching for top {k} similar artworks...\")\n",
    "    distances, indices = index.search(query_features, k)\n",
    "    print(\"  Search completed.\")\n",
    "    return distances[0], indices[0]\n",
    "\n",
    "def get_artwork_info(indices, image_df):\n",
    "    \"\"\"\n",
    "    Retrieves artwork information based on the indices.\n",
    "\n",
    "    Parameters:\n",
    "    - indices (np.ndarray): Indices of the top matches.\n",
    "    - image_df (pd.DataFrame): DataFrame containing image metadata.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: DataFrame of the top matching artworks.\n",
    "    \"\"\"\n",
    "    print(\"  Retrieving artwork information for the top matches...\")\n",
    "    try:\n",
    "        results = image_df.iloc[indices].reset_index(drop=True)\n",
    "        print(\"  Artwork information retrieved.\")\n",
    "    except Exception as e:\n",
    "        print(f\"  Error retrieving artwork information: {e}\")\n",
    "        results = pd.DataFrame()  # Return empty DataFrame on error\n",
    "    return results\n",
    "\n",
    "def identify_artwork(image_path_or_url, model, index, image_df, transform, device, k=5):\n",
    "    \"\"\"\n",
    "    Identifies the top k artworks similar to the query image.\n",
    "\n",
    "    Parameters:\n",
    "    - image_path_or_url (str): Path or URL of the query image.\n",
    "    - model (torch.nn.Module): Feature extraction model.\n",
    "    - index (faiss.Index): FAISS index.\n",
    "    - image_df (pd.DataFrame): DataFrame containing image metadata.\n",
    "    - transform (torchvision.transforms.Compose): Transformations to apply.\n",
    "    - device (torch.device): Device to send the image tensor.\n",
    "    - k (int): Number of top matches to retrieve.\n",
    "\n",
    "    Returns:\n",
    "    - results (pd.DataFrame): DataFrame of the top matching artworks.\n",
    "    - distances (np.ndarray): Similarity scores.\n",
    "    \"\"\"\n",
    "    print(\"\\nIdentifying artwork...\")\n",
    "    img_tensor = process_input_image(image_path_or_url, transform, device)\n",
    "    query_features = get_image_features(model, img_tensor)\n",
    "    distances, indices = search_artwork(query_features, index, k)\n",
    "    results = get_artwork_info(indices, image_df)\n",
    "    print(\"Artwork identification completed.\")\n",
    "    return results, distances\n",
    "\n",
    "def extract_features(model, dataloader, device):\n",
    "    \"\"\"\n",
    "    Extracts features from images using the provided model and dataloader.\n",
    "\n",
    "    Parameters:\n",
    "    - model (torch.nn.Module): The feature extraction model.\n",
    "    - dataloader (DataLoader): DataLoader for the dataset.\n",
    "    - device (torch.device): Device to perform computations on.\n",
    "\n",
    "    Returns:\n",
    "    - torch.Tensor: Extracted features.\n",
    "    - np.ndarray: Corresponding labels.\n",
    "    \"\"\"\n",
    "    print(\"Starting feature extraction...\")\n",
    "    features = []\n",
    "    labels = []\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (imgs, lbls) in enumerate(tqdm(dataloader, desc=\"Extracting features\"), 1):\n",
    "            imgs = imgs.to(device)\n",
    "            try:\n",
    "                outputs = model(imgs)\n",
    "                outputs = outputs.view(outputs.size(0), -1)  # Flatten to (batch_size, feature_dim)\n",
    "                outputs = outputs / outputs.norm(dim=1, keepdim=True)\n",
    "                features.append(outputs.cpu())\n",
    "                labels.extend(lbls.cpu().numpy())\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing batch {batch_idx}: {e}\")\n",
    "    features = torch.cat(features, dim=0)\n",
    "    labels = np.array(labels)\n",
    "    print(\"Feature extraction completed.\")\n",
    "    return features, labels\n",
    "\n",
    "# Step 1: Load and Prepare the WikiArt Dataset (All Classes)\n",
    "print(\"\\nStep 1: Loading and preparing the WikiArt dataset (all classes)...\")\n",
    "\n",
    "# Specify the path to the WikiArt dataset\n",
    "wikiart_dir = '../../scratch/mexas.v'\n",
    "\n",
    "# Get the list of class names (styles, artists, or genres)\n",
    "classes = [d for d in os.listdir(wikiart_dir) if os.path.isdir(os.path.join(wikiart_dir, d))]\n",
    "\n",
    "# Prepare a list to hold image paths and labels\n",
    "data = []\n",
    "\n",
    "for label in classes:\n",
    "    # Get all image paths for the current class\n",
    "    image_paths = glob.glob(os.path.join(wikiart_dir, label, '*.jpg'))\n",
    "    for path in image_paths:\n",
    "        data.append({\n",
    "            'image_path': path,\n",
    "            'label': label\n",
    "        })\n",
    "\n",
    "# Create a DataFrame\n",
    "image_df = pd.DataFrame(data)\n",
    "print(f\"Prepared image_df with {len(image_df)} entries from all classes.\")\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "image_df['encoded_label'] = label_encoder.fit_transform(image_df['label'])\n",
    "\n",
    "# Split the dataset into training, validation, and test sets\n",
    "print(\"\\nSplitting dataset into training, validation, and test sets...\")\n",
    "train_df, temp_df = train_test_split(\n",
    "    image_df, test_size=0.3, random_state=42, stratify=image_df['encoded_label']\n",
    ")\n",
    "val_df, test_df = train_test_split(\n",
    "    temp_df, test_size=0.5, random_state=42, stratify=temp_df['encoded_label']\n",
    ")\n",
    "print(f\"Training set: {len(train_df)} images\")\n",
    "print(f\"Validation set: {len(val_df)} images\")\n",
    "print(f\"Test set: {len(test_df)} images\")\n",
    "\n",
    "# Ensure there are no duplicates between the sets\n",
    "train_paths = set(train_df['image_path'])\n",
    "val_paths = set(val_df['image_path'])\n",
    "test_paths = set(test_df['image_path'])\n",
    "\n",
    "assert len(train_paths.intersection(val_paths)) == 0, \"Overlap between training and validation sets!\"\n",
    "assert len(train_paths.intersection(test_paths)) == 0, \"Overlap between training and test sets!\"\n",
    "assert len(val_paths.intersection(test_paths)) == 0, \"Overlap between validation and test sets!\"\n",
    "\n",
    "# Step 2: Define Image Transformations\n",
    "print(\"\\nStep 2: Defining image transformations...\")\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "print(\"Image transformations defined.\")\n",
    "\n",
    "# Step 3: Create Custom Dataset and DataLoader\n",
    "print(\"\\nStep 3: Creating custom datasets and data loaders...\")\n",
    "\n",
    "class ArtImageDataset(Dataset):\n",
    "    def __init__(self, image_df, transform=None):\n",
    "        self.image_df = image_df.reset_index(drop=True)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_df.loc[idx, 'image_path']\n",
    "        label = self.image_df.loc[idx, 'encoded_label']\n",
    "        try:\n",
    "            img = Image.open(img_path).convert('RGB')\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image {img_path}: {e}\")\n",
    "            img = Image.new('RGB', (224, 224), (0, 0, 0))\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img, label  # Return image and label\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = ArtImageDataset(train_df, transform=train_transform)\n",
    "val_dataset = ArtImageDataset(val_df, transform=val_transform)\n",
    "test_dataset = ArtImageDataset(test_df, transform=val_transform)\n",
    "\n",
    "# Create DataLoaders\n",
    "batch_size = 32  # Adjust based on your GPU memory\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=8)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=8)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=8)\n",
    "\n",
    "print(\"DataLoaders created for training, validation, and test sets.\")\n",
    "\n",
    "# Step 4: Set Up Device and Load Pre-trained Model\n",
    "print(\"\\nStep 4: Setting up the device and loading the pre-trained model...\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load the pre-trained ResNet-50 model\n",
    "model = models.resnet50(pretrained=True)\n",
    "\n",
    "# Modify the final layer for fine-tuning\n",
    "num_ftrs = model.fc.in_features\n",
    "num_classes = len(classes)\n",
    "model.fc = nn.Linear(num_ftrs, num_classes)\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "print(\"Pre-trained model loaded and modified for fine-tuning.\")\n",
    "\n",
    "# Step 5: Fine-Tune the Model\n",
    "print(\"\\nStep 5: Fine-tuning the model...\")\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "num_epochs = 4  # Adjust based on convergence\n",
    "best_val_acc = 0.0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "\n",
    "    for inputs, labels in tqdm(train_loader, desc=\"Training\"):\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "    epoch_loss = running_loss / len(train_dataset)\n",
    "    epoch_acc = running_corrects.double() / len(train_dataset)\n",
    "\n",
    "    print(f\"Training Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}\")\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_running_loss = 0.0\n",
    "    val_running_corrects = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(val_loader, desc=\"Validation\"):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            val_running_loss += loss.item() * inputs.size(0)\n",
    "            val_running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "    val_epoch_loss = val_running_loss / len(val_dataset)\n",
    "    val_epoch_acc = val_running_corrects.double() / len(val_dataset)\n",
    "\n",
    "    print(f\"Validation Loss: {val_epoch_loss:.4f} Acc: {val_epoch_acc:.4f}\")\n",
    "\n",
    "    # Checkpointing\n",
    "    if val_epoch_acc > best_val_acc:\n",
    "        best_val_acc = val_epoch_acc\n",
    "        torch.save(model.state_dict(), 'best_resnet50_wikiart_finetuned.pth')\n",
    "        print(\"Best model updated.\")\n",
    "\n",
    "print(f\"\\nTraining complete. Best Validation Accuracy: {best_val_acc:.4f}\")\n",
    "\n",
    "# Load the best model\n",
    "model.load_state_dict(torch.load('best_resnet50_wikiart_finetuned.pth'))\n",
    "model.eval()\n",
    "print(\"Best fine-tuned model loaded.\")\n",
    "\n",
    "# Step 6: Extract Features with Conditional Loading\n",
    "print(\"\\nStep 6: Extracting features from training set...\")\n",
    "\n",
    "features_file = 'wikiart_features_finetuned.npy'\n",
    "\n",
    "if os.path.exists(features_file):\n",
    "    print(f\"Feature file '{features_file}' found. Loading existing features...\")\n",
    "    try:\n",
    "        features_np = np.load(features_file)\n",
    "        print(f\"Loaded features with shape: {features_np.shape}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading features from '{features_file}': {e}\")\n",
    "        print(\"Proceeding to extract features anew.\")\n",
    "\n",
    "        # Extract features as the file could not be loaded\n",
    "        features, labels = extract_features(model, train_loader, device)\n",
    "        print(f\"Extracted features shape: {features.shape}\")\n",
    "\n",
    "        # Convert features to numpy array\n",
    "        features_np = features.numpy().astype('float32')\n",
    "\n",
    "        # Save features\n",
    "        np.save(features_file, features_np)\n",
    "        print(f\"Features extracted and saved to '{features_file}'.\")\n",
    "else:\n",
    "    print(f\"Feature file '{features_file}' not found. Extracting features...\")\n",
    "\n",
    "    # Extract features\n",
    "    features, labels = extract_features(model, train_loader, device)\n",
    "    print(f\"Extracted features shape: {features.shape}\")\n",
    "\n",
    "    # Convert features to numpy array\n",
    "    features_np = features.numpy().astype('float32')\n",
    "\n",
    "    # Save features\n",
    "    np.save(features_file, features_np)\n",
    "    print(f\"Features extracted and saved to '{features_file}'.\")\n",
    "\n",
    "# Step 7: Create FAISS Index with Extracted Features\n",
    "print(\"\\nStep 7: Creating FAISS index with extracted features...\")\n",
    "\n",
    "index_file = 'wikiart_faiss_index_finetuned.bin'\n",
    "\n",
    "if os.path.exists(index_file):\n",
    "    print(f\"FAISS index file '{index_file}' found. Loading existing index...\")\n",
    "    try:\n",
    "        index = faiss.read_index(index_file)\n",
    "        print(f\"Loaded FAISS index from '{index_file}'.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading FAISS index from '{index_file}': {e}\")\n",
    "        print(\"Proceeding to create a new FAISS index.\")\n",
    "\n",
    "        # Create FAISS index\n",
    "        index = faiss.IndexFlatIP(features_np.shape[1])\n",
    "        index.add(features_np)\n",
    "        faiss.write_index(index, index_file)\n",
    "        print(f\"New FAISS index created and saved to '{index_file}'.\")\n",
    "else:\n",
    "    print(f\"FAISS index file '{index_file}' not found. Creating a new index...\")\n",
    "\n",
    "    # Create FAISS index\n",
    "    index = faiss.IndexFlatIP(features_np.shape[1])\n",
    "    index.add(features_np)\n",
    "    faiss.write_index(index, index_file)\n",
    "    print(f\"FAISS index created and saved to '{index_file}'.\")\n",
    "\n",
    "# Step 8: Evaluate the Model on the Test Set\n",
    "print(\"\\nStep 8: Evaluating the model on the test set...\")\n",
    "\n",
    "test_features_file = 'wikiart_test_features_finetuned.npy'\n",
    "\n",
    "if os.path.exists(test_features_file):\n",
    "    print(f\"Feature file '{test_features_file}' found. Loading existing features...\")\n",
    "    try:\n",
    "        test_features_np = np.load(test_features_file)\n",
    "        print(f\"Loaded test features with shape: {test_features_np.shape}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading features from '{test_features_file}': {e}\")\n",
    "        print(\"Proceeding to extract test features anew.\")\n",
    "\n",
    "        # Extract features as the file could not be loaded\n",
    "        test_features, test_labels = extract_features(model, test_loader, device)\n",
    "        print(f\"Extracted test features shape: {test_features.shape}\")\n",
    "\n",
    "        # Convert features to numpy array\n",
    "        test_features_np = test_features.numpy().astype('float32')\n",
    "\n",
    "        # Save features\n",
    "        np.save(test_features_file, test_features_np)\n",
    "        print(f\"Test features extracted and saved to '{test_features_file}'.\")\n",
    "else:\n",
    "    print(f\"Feature file '{test_features_file}' not found. Extracting test features...\")\n",
    "\n",
    "    # Extract test features\n",
    "    test_features, test_labels = extract_features(model, test_loader, device)\n",
    "    print(f\"Extracted test features shape: {test_features.shape}\")\n",
    "\n",
    "    # Convert features to numpy array\n",
    "    test_features_np = test_features.numpy().astype('float32')\n",
    "\n",
    "    # Save features\n",
    "    np.save(test_features_file, test_features_np)\n",
    "    print(f\"Test features extracted and saved to '{test_features_file}'.\")\n",
    "\n",
    "# Perform similarity search for each test image\n",
    "print(\"\\nPerforming similarity search on the test set...\")\n",
    "\n",
    "k = 5  # Number of top matches to retrieve\n",
    "\n",
    "correct_at_1 = 0\n",
    "correct_at_k = 0\n",
    "\n",
    "for i in tqdm(range(len(test_features_np)), desc=\"Evaluating\"):\n",
    "    query_feature = test_features_np[i:i+1]  # Shape: (1, feature_dim)\n",
    "    true_label = test_df.iloc[i]['encoded_label']\n",
    "\n",
    "    distances, indices = index.search(query_feature, k)\n",
    "\n",
    "    retrieved_labels = train_df.iloc[indices[0]]['encoded_label'].values\n",
    "\n",
    "    if retrieved_labels[0] == true_label:\n",
    "        correct_at_1 += 1\n",
    "\n",
    "    if true_label in retrieved_labels:\n",
    "        correct_at_k += 1\n",
    "\n",
    "total_queries = len(test_features_np)\n",
    "top1_accuracy = correct_at_1 / total_queries * 100\n",
    "topk_accuracy = correct_at_k / total_queries * 100\n",
    "\n",
    "print(f\"\\nEvaluation Results:\")\n",
    "print(f\"Total Queries: {total_queries}\")\n",
    "print(f\"Top-1 Accuracy: {top1_accuracy:.2f}%\")\n",
    "print(f\"Top-{k} Accuracy: {topk_accuracy:.2f}%\")\n",
    "\n",
    "# Additional evaluation metrics can be added here if desired\n",
    "\n",
    "# Optional: Save evaluation results to a file\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbfdbd41-32a0-4b3c-8393-082373ef73de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FINAL",
   "language": "python",
   "name": "final"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
